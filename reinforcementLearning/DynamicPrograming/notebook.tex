
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Dynamic\_Programming}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Mini Project: Dynamic
Programming}\label{mini-project-dynamic-programming}

In this notebook, you will write your own implementations of many
classical dynamic programming algorithms.

While we have provided some starter code, you are welcome to erase these
hints and write your code from scratch.

    \subsubsection{Part 0: Explore
FrozenLakeEnv}\label{part-0-explore-frozenlakeenv}

Use the code cell below to create an instance of the
\href{https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py}{FrozenLake}
environment.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{!}pip install \PYZhy{}q \PY{n+nv}{matplotlib}\PY{o}{=}\PY{o}{=}\PY{l+m}{2}.2.2
        \PY{k+kn}{from} \PY{n+nn}{frozenlake} \PY{k}{import} \PY{n}{FrozenLakeEnv}
        
        \PY{n}{env} \PY{o}{=} \PY{n}{FrozenLakeEnv}\PY{p}{(}\PY{n}{is\PYZus{}slippery}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\textcolor{ansi-yellow}{You are using pip version 18.0, however version 18.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.}

    \end{Verbatim}

    The agent moves through a \(4 \times 4\) gridworld, with states numbered
as follows:

\begin{verbatim}
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]
\end{verbatim}

and the agent has 4 potential actions:

\begin{verbatim}
LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3
\end{verbatim}

Thus, \(\mathcal{S}^+ = \{0, 1, \ldots, 15\}\), and
\(\mathcal{A} = \{0, 1, 2, 3\}\). Verify this by running the code cell
below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} print the state space and action space}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{observation\PYZus{}space}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{action\PYZus{}space}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} print the total number of states and actions}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Discrete(16)
Discrete(4)
16
4

    \end{Verbatim}

    Dynamic programming assumes that the agent has full knowledge of the
MDP. We have already amended the \texttt{frozenlake.py} file to make the
one-step dynamics accessible to the agent.

Execute the code cell below to return the one-step dynamics
corresponding to a particular state and action. In particular,
\texttt{env.P{[}1{]}{[}0{]}} returns the the probability of each
possible reward and next state, if the agent is in state 1 of the
gridworld and decides to go left.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:} [(0.3333333333333333, 1, 0.0, False),
         (0.3333333333333333, 0, 0.0, False),
         (0.3333333333333333, 5, 0.0, True)]
\end{Verbatim}
            
    Each entry takes the form

\begin{verbatim}
prob, next_state, reward, done
\end{verbatim}

where: - \texttt{prob} details the conditional probability of the
corresponding (\texttt{next\_state}, \texttt{reward}) pair, and -
\texttt{done} is \texttt{True} if the \texttt{next\_state} is a terminal
state, and otherwise \texttt{False}.

Thus, we can interpret \texttt{env.P{[}1{]}{[}0{]}} as follows: \[
\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \begin{cases}
               \frac{1}{3} \text{ if } s'=1, r=0\\
               \frac{1}{3} \text{ if } s'=0, r=0\\
               \frac{1}{3} \text{ if } s'=5, r=0\\
               0 \text{ else}
            \end{cases}
\]

To understand the value of \texttt{env.P{[}1{]}{[}0{]}}, note that when
you create a FrozenLake environment, it takes as an (optional) argument
\texttt{is\_slippery}, which defaults to \texttt{True}.

To see this, change the first line in the notebook from
\texttt{env\ =\ FrozenLakeEnv()} to
\texttt{env\ =\ FrozenLakeEnv(is\_slippery=False)}. Then, when you check
\texttt{env.P{[}1{]}{[}0{]}}, it should look like what you expect (i.e.,
\texttt{env.P{[}1{]}{[}0{]}\ =\ {[}(1.0,\ 0,\ 0.0,\ False){]}}).

The default value for the \texttt{is\_slippery} argument is
\texttt{True}, and so \texttt{env\ =\ FrozenLakeEnv()} is equivalent to
\texttt{env\ =\ FrozenLakeEnv(is\_slippery=True)}. In the event that
\texttt{is\_slippery=True}, you see that this can result in the agent
moving in a direction that it did not intend (where the idea is that the
ground is \emph{slippery}, and so the agent can slide to a location
other than the one it wanted).

Feel free to change the code cell above to explore how the environment
behaves in response to other (state, action) pairs.

Before proceeding to the next part, make sure that you set
\texttt{is\_slippery=True}, so that your implementations below will work
with the slippery environment!

    \subsubsection{Part 1: Iterative Policy
Evaluation}\label{part-1-iterative-policy-evaluation}

In this section, you will write your own implementation of iterative
policy evaluation.

Your algorithm should accept four arguments as \textbf{input}: -
\texttt{env}: This is an instance of an OpenAI Gym environment, where
\texttt{env.P} returns the one-step dynamics. - \texttt{policy}: This is
a 2D numpy array with \texttt{policy.shape{[}0{]}} equal to the number
of states (\texttt{env.nS}), and \texttt{policy.shape{[}1{]}} equal to
the number of actions (\texttt{env.nA}). \texttt{policy{[}s{]}{[}a{]}}
returns the probability that the agent takes action \texttt{a} while in
state \texttt{s} under the policy. - \texttt{gamma}: This is the
discount rate. It must be a value between 0 and 1, inclusive (default
value: \texttt{1}). - \texttt{theta}: This is a very small positive
number that is used to decide if the estimate has sufficiently converged
to the true value function (default value: \texttt{1e-8}).

The algorithm returns as \textbf{output}: - \texttt{V}: This is a 1D
numpy array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s} under the input policy.

Please complete the function in the code cell below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{k}{def} \PY{n+nf}{policy\PYZus{}evaluation}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{policy}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{theta}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
            \PY{n}{V} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}
            
            \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                \PY{n}{delta} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{for} \PY{n}{state} \PY{o+ow}{in} \PY{n+nb}{list}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                    \PY{n}{v} \PY{o}{=} \PY{n}{V}\PY{p}{[}\PY{n}{state}\PY{p}{]}
                    \PY{n}{summatory\PYZus{}actions} \PY{o}{=} \PY{l+m+mi}{0}
                    \PY{k}{for} \PY{n}{action} \PY{o+ow}{in} \PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                        \PY{n}{summatory\PYZus{}sprime\PYZus{}reward} \PY{o}{=} \PY{l+m+mi}{0}
                        \PY{k}{for} \PY{n}{prob\PYZus{}next\PYZus{}state} \PY{o+ow}{in} \PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]}\PY{p}{:}
                            \PY{n}{summatory\PYZus{}sprime\PYZus{}reward} \PY{o}{=} \PY{n}{summatory\PYZus{}sprime\PYZus{}reward} \PY{o}{+} \PY{p}{(}\PY{n}{prob\PYZus{}next\PYZus{}state}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{prob\PYZus{}next\PYZus{}state}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{V}\PY{p}{[}\PY{n}{prob\PYZus{}next\PYZus{}state}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
                        \PY{n}{summatory\PYZus{}actions} \PY{o}{=} \PY{n}{summatory\PYZus{}actions} \PY{o}{+} \PY{n}{policy}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{[}\PY{n}{action}\PY{p}{]} \PY{o}{*} \PY{n}{summatory\PYZus{}sprime\PYZus{}reward}
        
                    \PY{n}{V}\PY{p}{[}\PY{n}{state}\PY{p}{]} \PY{o}{=} \PY{n}{summatory\PYZus{}actions}
                    \PY{n}{delta} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{delta}\PY{p}{,} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{v} \PY{o}{\PYZhy{}} \PY{n}{V}\PY{p}{[}\PY{n}{state}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                
                \PY{k}{if} \PY{n}{delta} \PY{o}{\PYZlt{}} \PY{n}{theta}\PY{p}{:}
                    \PY{k}{break}
        
            \PY{c+c1}{\PYZsh{}\PYZsh{} TODO: complete the function}
            
            \PY{k}{return} \PY{n}{V}
\end{Verbatim}


    We will evaluate the equiprobable random policy \(\pi\), where
\(\pi(a|s) = \frac{1}{|\mathcal{A}(s)|}\) for all \(s\in\mathcal{S}\)
and \(a\in\mathcal{A}(s)\).

Use the code cell below to specify this policy in the variable
\texttt{random\_policy}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{random\PYZus{}policy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}
\end{Verbatim}


    Run the next code cell to evaluate the equiprobable random policy and
visualize the output. The state-value function has been reshaped to
match the shape of the gridworld.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{from} \PY{n+nn}{plot\PYZus{}utils} \PY{k}{import} \PY{n}{plot\PYZus{}values}
        
        \PY{c+c1}{\PYZsh{} evaluate the policy }
        \PY{n}{V} \PY{o}{=} \PY{n}{policy\PYZus{}evaluation}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{random\PYZus{}policy}\PY{p}{)}
        
        \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Run the code cell below to test your function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly!

\textbf{Note:} In order to ensure accurate results, make sure that your
\texttt{policy\_evaluation} function satisfies the requirements outlined
above (with four inputs, a single output, and with the default values of
the input arguments unchanged).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{import} \PY{n+nn}{check\PYZus{}test}
        
        \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{policy\PYZus{}evaluation\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{policy\PYZus{}evaluation}\PY{p}{)}
\end{Verbatim}


    \textbf{{PASSED}}

    
    \subsubsection{\texorpdfstring{Part 2: Obtain \(q_\pi\) from
\(v_\pi\)}{Part 2: Obtain q\_\textbackslash{}pi from v\_\textbackslash{}pi}}\label{part-2-obtain-q_pi-from-v_pi}

In this section, you will write a function that takes the state-value
function estimate as input, along with some state \(s\in\mathcal{S}\).
It returns the \textbf{row in the action-value function} corresponding
to the input state \(s\in\mathcal{S}\). That is, your function should
accept as input both \(v_\pi\) and \(s\), and return \(q_\pi(s,a)\) for
all \(a\in\mathcal{A}(s)\).

Your algorithm should accept four arguments as \textbf{input}: -
\texttt{env}: This is an instance of an OpenAI Gym environment, where
\texttt{env.P} returns the one-step dynamics. - \texttt{V}: This is a 1D
numpy array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}. - \texttt{s}: This is an integer corresponding to a
state in the environment. It should be a value between \texttt{0} and
\texttt{(env.nS)-1}, inclusive. - \texttt{gamma}: This is the discount
rate. It must be a value between 0 and 1, inclusive (default value:
\texttt{1}).

The algorithm returns as \textbf{output}: - \texttt{q}: This is a 1D
numpy array with \texttt{q.shape{[}0{]}} equal to the number of actions
(\texttt{env.nA}). \texttt{q{[}a{]}} contains the (estimated) value of
state \texttt{s} and action \texttt{a}.

Please complete the function in the code cell below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{def} \PY{n+nf}{q\PYZus{}from\PYZus{}v}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
            \PY{n}{q} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}
            
            \PY{n}{action\PYZus{}value} \PY{o}{=} \PY{l+m+mi}{0}
            \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{p}{[}\PY{n}{s}\PY{p}{]}\PY{p}{:}
                \PY{n}{value\PYZus{}action\PYZus{}funct} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{k}{for} \PY{n}{prob\PYZus{}next\PYZus{}state} \PY{o+ow}{in} \PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{p}{[}\PY{n}{s}\PY{p}{]}\PY{p}{[}\PY{n}{a}\PY{p}{]}\PY{p}{:}
                    \PY{n}{value\PYZus{}action\PYZus{}funct} \PY{o}{+}\PY{o}{=} \PY{n}{prob\PYZus{}next\PYZus{}state}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{prob\PYZus{}next\PYZus{}state}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{+} \PY{p}{(}\PY{n}{gamma} \PY{o}{*} \PY{n}{V}\PY{p}{[}\PY{n}{prob\PYZus{}next\PYZus{}state}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                    
                \PY{n}{q}\PY{p}{[}\PY{n}{a}\PY{p}{]} \PY{o}{=} \PY{n}{value\PYZus{}action\PYZus{}funct}
            
            \PY{k}{return} \PY{n}{q}
\end{Verbatim}


    Run the code cell below to print the action-value function corresponding
to the above state-value function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{Q} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{]}\PY{p}{)}
         \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}\PY{p}{:}
             \PY{n}{Q}\PY{p}{[}\PY{n}{s}\PY{p}{]} \PY{o}{=} \PY{n}{q\PYZus{}from\PYZus{}v}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{s}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Action\PYZhy{}Value Function:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{Q}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Action-Value Function:
[[0.0147094  0.01393978 0.01393978 0.01317015]
 [0.00852356 0.01163091 0.0108613  0.01550788]
 [0.02444514 0.02095298 0.02406033 0.01435346]
 [0.01047649 0.01047649 0.00698432 0.01396865]
 [0.02166487 0.01701828 0.01624865 0.01006281]
 [0.         0.         0.         0.        ]
 [0.05433538 0.04735105 0.05433538 0.00698432]
 [0.         0.         0.         0.        ]
 [0.01701828 0.04099204 0.03480619 0.04640826]
 [0.07020885 0.11755991 0.10595784 0.05895312]
 [0.18940421 0.17582037 0.16001424 0.04297382]
 [0.         0.         0.         0.        ]
 [0.         0.         0.         0.        ]
 [0.08799677 0.20503718 0.23442716 0.17582037]
 [0.25238823 0.53837051 0.52711478 0.43929118]
 [0.         0.         0.         0.        ]]

    \end{Verbatim}

    Run the code cell below to test your function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly!

\textbf{Note:} In order to ensure accurate results, make sure that the
\texttt{q\_from\_v} function satisfies the requirements outlined above
(with four inputs, a single output, and with the default values of the
input arguments unchanged).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{q\PYZus{}from\PYZus{}v\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{q\PYZus{}from\PYZus{}v}\PY{p}{)}
\end{Verbatim}


    \textbf{{PASSED}}

    
    \subsubsection{Part 3: Policy
Improvement}\label{part-3-policy-improvement}

In this section, you will write your own implementation of policy
improvement.

Your algorithm should accept three arguments as \textbf{input}: -
\texttt{env}: This is an instance of an OpenAI Gym environment, where
\texttt{env.P} returns the one-step dynamics. - \texttt{V}: This is a 1D
numpy array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}. - \texttt{gamma}: This is the discount rate. It must
be a value between 0 and 1, inclusive (default value: \texttt{1}).

The algorithm returns as \textbf{output}: - \texttt{policy}: This is a
2D numpy array with \texttt{policy.shape{[}0{]}} equal to the number of
states (\texttt{env.nS}), and \texttt{policy.shape{[}1{]}} equal to the
number of actions (\texttt{env.nA}). \texttt{policy{[}s{]}{[}a{]}}
returns the probability that the agent takes action \texttt{a} while in
state \texttt{s} under the policy.

Please complete the function in the code cell below. You are encouraged
to use the \texttt{q\_from\_v} function you implemented above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{policy\PYZus{}improvement}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             \PY{n}{policy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}
             
             \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}\PY{p}{:}
                 \PY{n}{Q} \PY{o}{=} \PY{n}{q\PYZus{}from\PYZus{}v}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{s}\PY{p}{)}
                 \PY{n}{policy}\PY{p}{[}\PY{n}{s}\PY{p}{]}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{Q}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
         
             \PY{k}{return} \PY{n}{policy}
\end{Verbatim}


    Run the code cell below to test your function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly!

\textbf{Note:} In order to ensure accurate results, make sure that the
\texttt{policy\_improvement} function satisfies the requirements
outlined above (with three inputs, a single output, and with the default
values of the input arguments unchanged).

Before moving on to the next part of the notebook, you are strongly
encouraged to check out the solution in
\textbf{Dynamic\_Programming\_Solution.ipynb}. There are many correct
ways to approach this function!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{policy\PYZus{}improvement\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{policy\PYZus{}improvement}\PY{p}{)}
\end{Verbatim}


    \textbf{{PASSED}}

    
    \subsubsection{Part 4: Policy Iteration}\label{part-4-policy-iteration}

In this section, you will write your own implementation of policy
iteration. The algorithm returns the optimal policy, along with its
corresponding state-value function.

Your algorithm should accept three arguments as \textbf{input}: -
\texttt{env}: This is an instance of an OpenAI Gym environment, where
\texttt{env.P} returns the one-step dynamics. - \texttt{gamma}: This is
the discount rate. It must be a value between 0 and 1, inclusive
(default value: \texttt{1}). - \texttt{theta}: This is a very small
positive number that is used to decide if the policy evaluation step has
sufficiently converged to the true value function (default value:
\texttt{1e-8}).

The algorithm returns as \textbf{output}: - \texttt{policy}: This is a
2D numpy array with \texttt{policy.shape{[}0{]}} equal to the number of
states (\texttt{env.nS}), and \texttt{policy.shape{[}1{]}} equal to the
number of actions (\texttt{env.nA}). \texttt{policy{[}s{]}{[}a{]}}
returns the probability that the agent takes action \texttt{a} while in
state \texttt{s} under the policy. - \texttt{V}: This is a 1D numpy
array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}.

Please complete the function in the code cell below. You are strongly
encouraged to use the \texttt{policy\_evaluation} and
\texttt{policy\_improvement} functions you implemented above.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{import} \PY{n+nn}{copy}
         
         \PY{k}{def} \PY{n+nf}{policy\PYZus{}iteration}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{theta}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
             \PY{n}{policy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}
             \PY{n}{V} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}
             \PY{n}{policy\PYZus{}stable} \PY{o}{=} \PY{k+kc}{False}
             
             \PY{k}{while} \PY{n}{policy\PYZus{}stable} \PY{o+ow}{is} \PY{k+kc}{False}\PY{p}{:}
                 \PY{n}{V} \PY{o}{=} \PY{n}{policy\PYZus{}evaluation}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{policy}\PY{p}{,} \PY{n}{gamma}\PY{p}{,} \PY{n}{theta}\PY{p}{)}
                 \PY{n}{policy\PYZus{}prime} \PY{o}{=} \PY{n}{policy\PYZus{}improvement}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}
                 
                 \PY{k}{if} \PY{n}{policy\PYZus{}prime}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{policy}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                     \PY{n}{policy\PYZus{}stable} \PY{o}{=} \PY{k+kc}{True}
         
                 \PY{n}{policy} \PY{o}{=} \PY{n}{policy\PYZus{}prime}
             
             \PY{k}{return} \PY{n}{policy}\PY{p}{,} \PY{n}{V}
\end{Verbatim}


    Run the next code cell to solve the MDP and visualize the output. The
optimal state-value function has been reshaped to match the shape of the
gridworld.

\textbf{Compare the optimal state-value function to the state-value
function from Part 1 of this notebook}. \emph{Is the optimal state-value
function consistently greater than or equal to the state-value function
for the equiprobable random policy?}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} obtain the optimal policy and optimal state\PYZhy{}value function}
         \PY{n}{policy\PYZus{}pi}\PY{p}{,} \PY{n}{V\PYZus{}pi} \PY{o}{=} \PY{n}{policy\PYZus{}iteration}\PY{p}{(}\PY{n}{env}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the optimal policy}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy\PYZus{}pi}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}pi}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):
[[1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]] 


    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Run the code cell below to test your function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly!

\textbf{Note:} In order to ensure accurate results, make sure that the
\texttt{policy\_iteration} function satisfies the requirements outlined
above (with three inputs, two outputs, and with the default values of
the input arguments unchanged).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{policy\PYZus{}iteration\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{policy\PYZus{}iteration}\PY{p}{)}
\end{Verbatim}


    \textbf{{PASSED}}

    
    \subsubsection{Part 5: Truncated Policy
Iteration}\label{part-5-truncated-policy-iteration}

In this section, you will write your own implementation of truncated
policy iteration.

You will begin by implementing truncated policy evaluation. Your
algorithm should accept five arguments as \textbf{input}: -
\texttt{env}: This is an instance of an OpenAI Gym environment, where
\texttt{env.P} returns the one-step dynamics. - \texttt{policy}: This is
a 2D numpy array with \texttt{policy.shape{[}0{]}} equal to the number
of states (\texttt{env.nS}), and \texttt{policy.shape{[}1{]}} equal to
the number of actions (\texttt{env.nA}). \texttt{policy{[}s{]}{[}a{]}}
returns the probability that the agent takes action \texttt{a} while in
state \texttt{s} under the policy. - \texttt{V}: This is a 1D numpy
array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}. - \texttt{max\_it}: This is a positive integer that
corresponds to the number of sweeps through the state space (default
value: \texttt{1}). - \texttt{gamma}: This is the discount rate. It must
be a value between 0 and 1, inclusive (default value: \texttt{1}).

The algorithm returns as \textbf{output}: - \texttt{V}: This is a 1D
numpy array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}.

Please complete the function in the code cell below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{truncated\PYZus{}policy\PYZus{}evaluation}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{policy}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{max\PYZus{}it}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             \PY{n}{counter\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{k}{while} \PY{n}{counter\PYZus{}iter} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}it}\PY{p}{:}
                 \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}\PY{p}{:}
                     \PY{n}{v} \PY{o}{=} \PY{l+m+mi}{0}
                     \PY{n}{q} \PY{o}{=} \PY{n}{q\PYZus{}from\PYZus{}v}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{s}\PY{p}{)}
                     \PY{k}{for} \PY{n}{a}\PY{p}{,} \PY{n}{action\PYZus{}prob} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{policy}\PY{p}{[}\PY{n}{s}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                         \PY{n}{v} \PY{o}{+}\PY{o}{=} \PY{n}{action\PYZus{}prob} \PY{o}{*} \PY{n}{q}\PY{p}{[}\PY{n}{a}\PY{p}{]}
                     \PY{n}{V}\PY{p}{[}\PY{n}{s}\PY{p}{]} \PY{o}{=} \PY{n}{v}    
                 \PY{n}{counter\PYZus{}iter} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             
             \PY{k}{return} \PY{n}{V}
\end{Verbatim}


    Next, you will implement truncated policy iteration. Your algorithm
should accept five arguments as \textbf{input}: - \texttt{env}: This is
an instance of an OpenAI Gym environment, where \texttt{env.P} returns
the one-step dynamics. - \texttt{max\_it}: This is a positive integer
that corresponds to the number of sweeps through the state space
(default value: \texttt{1}). - \texttt{gamma}: This is the discount
rate. It must be a value between 0 and 1, inclusive (default value:
\texttt{1}). - \texttt{theta}: This is a very small positive number that
is used for the stopping criterion (default value: \texttt{1e-8}).

The algorithm returns as \textbf{output}: - \texttt{policy}: This is a
2D numpy array with \texttt{policy.shape{[}0{]}} equal to the number of
states (\texttt{env.nS}), and \texttt{policy.shape{[}1{]}} equal to the
number of actions (\texttt{env.nA}). \texttt{policy{[}s{]}{[}a{]}}
returns the probability that the agent takes action \texttt{a} while in
state \texttt{s} under the policy. - \texttt{V}: This is a 1D numpy
array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}.

Please complete the function in the code cell below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{def} \PY{n+nf}{truncated\PYZus{}policy\PYZus{}iteration}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{max\PYZus{}it}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{theta}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
             \PY{n}{V} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}
             \PY{n}{Vold} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}
             \PY{n}{policy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{,} \PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{]}\PY{p}{)} \PY{o}{/} \PY{n}{env}\PY{o}{.}\PY{n}{nA}
             
             \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                 \PY{n}{policy} \PY{o}{=} \PY{n}{policy\PYZus{}improvement}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{)}
                 \PY{n}{Vold} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{V}\PY{p}{)}
                 \PY{n}{V} \PY{o}{=} \PY{n}{truncated\PYZus{}policy\PYZus{}evaluation}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{policy}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{max\PYZus{}it}\PY{p}{)}
                 \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{amax}\PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{V} \PY{o}{\PYZhy{}} \PY{n}{Vold}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{theta}\PY{p}{:}
                     \PY{k}{break}
             
             \PY{k}{return} \PY{n}{policy}\PY{p}{,} \PY{n}{V}
\end{Verbatim}


    Run the next code cell to solve the MDP and visualize the output. The
state-value function has been reshaped to match the shape of the
gridworld.

Play with the value of the \texttt{max\_it} argument. Do you always end
with the optimal state-value function?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{policy\PYZus{}tpi}\PY{p}{,} \PY{n}{V\PYZus{}tpi} \PY{o}{=} \PY{n}{truncated\PYZus{}policy\PYZus{}iteration}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{max\PYZus{}it}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the optimal policy}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy\PYZus{}tpi}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the optimal state\PYZhy{}value function}
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}tpi}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):
[[1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]] 


    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Run the code cell below to test your function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly!

\textbf{Note:} In order to ensure accurate results, make sure that the
\texttt{truncated\_policy\_iteration} function satisfies the
requirements outlined above (with four inputs, two outputs, and with the
default values of the input arguments unchanged).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{truncated\PYZus{}policy\PYZus{}iteration\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{truncated\PYZus{}policy\PYZus{}iteration}\PY{p}{)}
\end{Verbatim}


    \textbf{{PASSED}}

    
    \subsubsection{Part 6: Value Iteration}\label{part-6-value-iteration}

In this section, you will write your own implementation of value
iteration.

Your algorithm should accept three arguments as input: - \texttt{env}:
This is an instance of an OpenAI Gym environment, where \texttt{env.P}
returns the one-step dynamics. - \texttt{gamma}: This is the discount
rate. It must be a value between 0 and 1, inclusive (default value:
\texttt{1}). - \texttt{theta}: This is a very small positive number that
is used for the stopping criterion (default value: \texttt{1e-8}).

The algorithm returns as \textbf{output}: - \texttt{policy}: This is a
2D numpy array with \texttt{policy.shape{[}0{]}} equal to the number of
states (\texttt{env.nS}), and \texttt{policy.shape{[}1{]}} equal to the
number of actions (\texttt{env.nA}). \texttt{policy{[}s{]}{[}a{]}}
returns the probability that the agent takes action \texttt{a} while in
state \texttt{s} under the policy. - \texttt{V}: This is a 1D numpy
array with \texttt{V.shape{[}0{]}} equal to the number of states
(\texttt{env.nS}). \texttt{V{[}s{]}} contains the estimated value of
state \texttt{s}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k}{def} \PY{n+nf}{value\PYZus{}iteration}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{theta}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
             \PY{n}{V} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nS}\PY{p}{)}
             \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{env}\PY{o}{.}\PY{n}{nA}\PY{p}{)}
             
             \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                 \PY{n}{delta} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{env}\PY{o}{.}\PY{n}{P}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                     \PY{n}{v} \PY{o}{=} \PY{n}{V}\PY{p}{[}\PY{n}{s}\PY{p}{]}
                     \PY{n}{V}\PY{p}{[}\PY{n}{s}\PY{p}{]} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{q\PYZus{}from\PYZus{}v}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{,} \PY{n}{s}\PY{p}{,} \PY{n}{gamma}\PY{p}{)}\PY{p}{)}
                     \PY{n}{delta} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{delta}\PY{p}{,} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{v}\PY{o}{\PYZhy{}}\PY{n}{V}\PY{p}{[}\PY{n}{s}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{k}{if} \PY{n}{delta} \PY{o}{\PYZlt{}} \PY{n}{theta}\PY{p}{:}
                     \PY{k}{break}
                     
             \PY{k}{return} \PY{n}{policy\PYZus{}improvement}\PY{p}{(}\PY{n}{env}\PY{p}{,} \PY{n}{V}\PY{p}{)}\PY{p}{,} \PY{n}{V}
             
             \PY{c+c1}{\PYZsh{}return policy, V}
\end{Verbatim}


    Use the next code cell to solve the MDP and visualize the output. The
state-value function has been reshaped to match the shape of the
gridworld.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{policy\PYZus{}vi}\PY{p}{,} \PY{n}{V\PYZus{}vi} \PY{o}{=} \PY{n}{value\PYZus{}iteration}\PY{p}{(}\PY{n}{env}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print the optimal policy}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{policy\PYZus{}vi}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the optimal state\PYZhy{}value function}
         \PY{n}{plot\PYZus{}values}\PY{p}{(}\PY{n}{V\PYZus{}vi}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Optimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):
[[1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]] 


    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Run the code cell below to test your function. If the code cell returns
\textbf{PASSED}, then you have implemented the function correctly!

\textbf{Note:} In order to ensure accurate results, make sure that the
\texttt{value\_iteration} function satisfies the requirements outlined
above (with three inputs, two outputs, and with the default values of
the input arguments unchanged).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{check\PYZus{}test}\PY{o}{.}\PY{n}{run\PYZus{}check}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value\PYZus{}iteration\PYZus{}check}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{value\PYZus{}iteration}\PY{p}{)}
\end{Verbatim}


    \textbf{{PASSED}}

    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
