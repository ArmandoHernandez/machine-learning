{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building powerful image classification models using very little data\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    # rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('data/train/cats/cat.0.jpg') # this is a PIL image\n",
    "x = img_to_array(img) # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape) # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to t he 'preview/' directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                         save_to_dir='preview', save_prefix='cat',\n",
    "                         save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 14s - loss: 0.7131 - acc: 0.5000 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 13s - loss: 0.6907 - acc: 0.5760 - val_loss: 0.6465 - val_acc: 0.6450\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 13s - loss: 0.6313 - acc: 0.6365 - val_loss: 0.6266 - val_acc: 0.6275\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 13s - loss: 0.6001 - acc: 0.6715 - val_loss: 0.5923 - val_acc: 0.6737\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 13s - loss: 0.5819 - acc: 0.7070 - val_loss: 0.5972 - val_acc: 0.6987- los - ETA: 0s - loss: 0.5837 - acc: 0.70\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 13s - loss: 0.5665 - acc: 0.7160 - val_loss: 0.5794 - val_acc: 0.6700\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 13s - loss: 0.5482 - acc: 0.7325 - val_loss: 0.5560 - val_acc: 0.7262\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 13s - loss: 0.5307 - acc: 0.7395 - val_loss: 0.5122 - val_acc: 0.7462\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 13s - loss: 0.5241 - acc: 0.7375 - val_loss: 0.5252 - val_acc: 0.7400\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 13s - loss: 0.5018 - acc: 0.7675 - val_loss: 0.4838 - val_acc: 0.7863\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 13s - loss: 0.5053 - acc: 0.7660 - val_loss: 0.5186 - val_acc: 0.7338\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 13s - loss: 0.5046 - acc: 0.7630 - val_loss: 0.4941 - val_acc: 0.7588\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 14s - loss: 0.4939 - acc: 0.7665 - val_loss: 0.5028 - val_acc: 0.7675\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 13s - loss: 0.4809 - acc: 0.7830 - val_loss: 0.5580 - val_acc: 0.7163\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 13s - loss: 0.4717 - acc: 0.7760 - val_loss: 0.5810 - val_acc: 0.7175\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 13s - loss: 0.4906 - acc: 0.7655 - val_loss: 0.4883 - val_acc: 0.7812\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 13s - loss: 0.4711 - acc: 0.7855 - val_loss: 0.4917 - val_acc: 0.7600\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 13s - loss: 0.4610 - acc: 0.7870 - val_loss: 0.5374 - val_acc: 0.7500c:\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 13s - loss: 0.4580 - acc: 0.7990 - val_loss: 0.5095 - val_acc: 0.7700\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 14s - loss: 0.4500 - acc: 0.8025 - val_loss: 0.6150 - val_acc: 0.7137\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 13s - loss: 0.4559 - acc: 0.8060 - val_loss: 0.6806 - val_acc: 0.7163\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 13s - loss: 0.4503 - acc: 0.8005 - val_loss: 0.4804 - val_acc: 0.7712\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 14s - loss: 0.4352 - acc: 0.8015 - val_loss: 0.4739 - val_acc: 0.7688\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 13s - loss: 0.4400 - acc: 0.8085 - val_loss: 0.5225 - val_acc: 0.7925\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 13s - loss: 0.4378 - acc: 0.8055 - val_loss: 0.5326 - val_acc: 0.7700\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 13s - loss: 0.4290 - acc: 0.8205 - val_loss: 0.4475 - val_acc: 0.8013\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 13s - loss: 0.4337 - acc: 0.8050 - val_loss: 0.6199 - val_acc: 0.7488\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 13s - loss: 0.4231 - acc: 0.8115 - val_loss: 0.4842 - val_acc: 0.7963\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 14s - loss: 0.4159 - acc: 0.8110 - val_loss: 0.4790 - val_acc: 0.7950\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 14s - loss: 0.4238 - acc: 0.8160 - val_loss: 0.5883 - val_acc: 0.7675\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 14s - loss: 0.4289 - acc: 0.8140 - val_loss: 0.4507 - val_acc: 0.8037\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 13s - loss: 0.4394 - acc: 0.8115 - val_loss: 0.5716 - val_acc: 0.7925\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 13s - loss: 0.4172 - acc: 0.8120 - val_loss: 0.4668 - val_acc: 0.8300\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 13s - loss: 0.3989 - acc: 0.8180 - val_loss: 0.5544 - val_acc: 0.7625\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 13s - loss: 0.4095 - acc: 0.8285 - val_loss: 0.4842 - val_acc: 0.7987\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 14s - loss: 0.4027 - acc: 0.8255 - val_loss: 0.5555 - val_acc: 0.7625\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 13s - loss: 0.4021 - acc: 0.8310 - val_loss: 0.4700 - val_acc: 0.7950\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 13s - loss: 0.4038 - acc: 0.8155 - val_loss: 0.5143 - val_acc: 0.7800\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 14s - loss: 0.4101 - acc: 0.8345 - val_loss: 0.5894 - val_acc: 0.7562\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 13s - loss: 0.4082 - acc: 0.8340 - val_loss: 0.4855 - val_acc: 0.8087\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 13s - loss: 0.4231 - acc: 0.8240 - val_loss: 0.5348 - val_acc: 0.7812\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 13s - loss: 0.4080 - acc: 0.8425 - val_loss: 0.4903 - val_acc: 0.7775\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 13s - loss: 0.3944 - acc: 0.8320 - val_loss: 0.5688 - val_acc: 0.7362\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 13s - loss: 0.4326 - acc: 0.8250 - val_loss: 0.5038 - val_acc: 0.7875\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 14s - loss: 0.3884 - acc: 0.8365 - val_loss: 0.4809 - val_acc: 0.8025 - lo\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 13s - loss: 0.3716 - acc: 0.8470 - val_loss: 0.5377 - val_acc: 0.7925\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 13s - loss: 0.3966 - acc: 0.8315 - val_loss: 0.5343 - val_acc: 0.7950\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 13s - loss: 0.3766 - acc: 0.8425 - val_loss: 0.5106 - val_acc: 0.8087\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 13s - loss: 0.4035 - acc: 0.8415 - val_loss: 0.5222 - val_acc: 0.7825\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 13s - loss: 0.3913 - acc: 0.8445 - val_loss: 0.6228 - val_acc: 0.7925\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(150,150,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten()) # this converts our 3D feature maps to 1D features vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolders of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train', # this is the target directory\n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary') # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'data/validation',\n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2000 // batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=800 // batch_size)\n",
    "model.save_weights('first_try.h5') # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "dataget = ImageDataGenerator(rescale=1./255)\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "generator = dataget.flow_from_directory('data/train',\n",
    "                                       target_size=(150,150),\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode=None, # this means our generator will yield only batches of data, no labels\n",
    "                                       shuffle=False) # our data will be in order, so all first 1000 images will be cats, then 1000 dogs-cats.ipynb\n",
    "# the predict_generator method returns the output of a model, given\n",
    "# a generator that yields batches of numpy data\n",
    "bottleneck_features_train = model.predict_generator(generator, (2000 // batch_size))\n",
    "\n",
    "# save the output as numpy arrays\n",
    "#np.save(open('bottleneck_features_train.npy', 'w'), np.array2string(bottleneck_features_train, precision=8))\n",
    "with open('bottleneck_features_train.npy', 'wb') as features_train_file:\n",
    "    np.save(features_train_file, bottleneck_features_train)\n",
    "\n",
    "generator = datagen.flow_from_directory('data/validation',\n",
    "                                       target_size=(150,150),\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode=None,\n",
    "                                       shuffle=False)\n",
    "bottleneck_features_validation = model.predict_generator(generator, (800 // batch_size))\n",
    "#np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n",
    "with open('bottleneck_features_validation.npy', 'wb') as features_validation_file:\n",
    "    np.save(features_validation_file, bottleneck_features_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "0s - loss: 0.6468 - acc: 0.7580 - val_loss: 3.0678 - val_acc: 0.7837\n",
      "Epoch 2/50\n",
      "0s - loss: 0.3541 - acc: 0.8490 - val_loss: 3.3112 - val_acc: 0.7812\n",
      "Epoch 3/50\n",
      "0s - loss: 0.3055 - acc: 0.8845 - val_loss: 4.5587 - val_acc: 0.7025\n",
      "Epoch 4/50\n",
      "0s - loss: 0.2896 - acc: 0.8880 - val_loss: 3.0158 - val_acc: 0.8063\n",
      "Epoch 5/50\n",
      "0s - loss: 0.2272 - acc: 0.9115 - val_loss: 2.8105 - val_acc: 0.8150\n",
      "Epoch 6/50\n",
      "0s - loss: 0.1954 - acc: 0.9310 - val_loss: 3.7945 - val_acc: 0.7550\n",
      "Epoch 7/50\n",
      "0s - loss: 0.1825 - acc: 0.9325 - val_loss: 3.2694 - val_acc: 0.7900\n",
      "Epoch 8/50\n",
      "0s - loss: 0.1622 - acc: 0.9380 - val_loss: 3.2554 - val_acc: 0.7925\n",
      "Epoch 9/50\n",
      "0s - loss: 0.1513 - acc: 0.9460 - val_loss: 3.3945 - val_acc: 0.7788\n",
      "Epoch 10/50\n",
      "0s - loss: 0.1189 - acc: 0.9545 - val_loss: 2.9388 - val_acc: 0.8137\n",
      "Epoch 11/50\n",
      "0s - loss: 0.1377 - acc: 0.9580 - val_loss: 3.1308 - val_acc: 0.8013\n",
      "Epoch 12/50\n",
      "0s - loss: 0.0924 - acc: 0.9685 - val_loss: 2.7782 - val_acc: 0.8225\n",
      "Epoch 13/50\n",
      "0s - loss: 0.0984 - acc: 0.9690 - val_loss: 3.4386 - val_acc: 0.7788\n",
      "Epoch 14/50\n",
      "0s - loss: 0.0900 - acc: 0.9665 - val_loss: 3.1094 - val_acc: 0.8000\n",
      "Epoch 15/50\n",
      "0s - loss: 0.0651 - acc: 0.9715 - val_loss: 3.3356 - val_acc: 0.7875\n",
      "Epoch 16/50\n",
      "0s - loss: 0.0798 - acc: 0.9735 - val_loss: 3.2967 - val_acc: 0.7913\n",
      "Epoch 17/50\n",
      "0s - loss: 0.0720 - acc: 0.9745 - val_loss: 3.3177 - val_acc: 0.7863\n",
      "Epoch 18/50\n",
      "0s - loss: 0.0476 - acc: 0.9830 - val_loss: 3.0915 - val_acc: 0.8025\n",
      "Epoch 19/50\n",
      "0s - loss: 0.0626 - acc: 0.9750 - val_loss: 3.2082 - val_acc: 0.7963\n",
      "Epoch 20/50\n",
      "0s - loss: 0.0564 - acc: 0.9780 - val_loss: 2.8481 - val_acc: 0.8175\n",
      "Epoch 21/50\n",
      "0s - loss: 0.0567 - acc: 0.9770 - val_loss: 2.9614 - val_acc: 0.8125\n",
      "Epoch 22/50\n",
      "0s - loss: 0.0385 - acc: 0.9865 - val_loss: 2.9142 - val_acc: 0.8100\n",
      "Epoch 23/50\n",
      "0s - loss: 0.0323 - acc: 0.9890 - val_loss: 3.1388 - val_acc: 0.8000\n",
      "Epoch 24/50\n",
      "0s - loss: 0.0433 - acc: 0.9860 - val_loss: 2.9255 - val_acc: 0.8125\n",
      "Epoch 25/50\n",
      "0s - loss: 0.0417 - acc: 0.9875 - val_loss: 2.9973 - val_acc: 0.8113\n",
      "Epoch 26/50\n",
      "0s - loss: 0.0329 - acc: 0.9880 - val_loss: 3.0240 - val_acc: 0.8100\n",
      "Epoch 27/50\n",
      "0s - loss: 0.0335 - acc: 0.9895 - val_loss: 2.8054 - val_acc: 0.8225\n",
      "Epoch 28/50\n",
      "0s - loss: 0.0353 - acc: 0.9905 - val_loss: 2.7232 - val_acc: 0.8275\n",
      "Epoch 29/50\n",
      "0s - loss: 0.0239 - acc: 0.9915 - val_loss: 2.4489 - val_acc: 0.8450\n",
      "Epoch 30/50\n",
      "0s - loss: 0.0367 - acc: 0.9885 - val_loss: 2.6321 - val_acc: 0.8325\n",
      "Epoch 31/50\n",
      "0s - loss: 0.0069 - acc: 0.9975 - val_loss: 2.9339 - val_acc: 0.8150\n",
      "Epoch 32/50\n",
      "0s - loss: 0.0203 - acc: 0.9930 - val_loss: 2.7961 - val_acc: 0.8237\n",
      "Epoch 33/50\n",
      "0s - loss: 0.0406 - acc: 0.9900 - val_loss: 2.7520 - val_acc: 0.8263\n",
      "Epoch 34/50\n",
      "0s - loss: 0.0215 - acc: 0.9920 - val_loss: 2.9581 - val_acc: 0.8137\n",
      "Epoch 35/50\n",
      "0s - loss: 0.0171 - acc: 0.9940 - val_loss: 2.9011 - val_acc: 0.8150\n",
      "Epoch 36/50\n",
      "0s - loss: 0.0089 - acc: 0.9975 - val_loss: 2.9489 - val_acc: 0.8137\n",
      "Epoch 37/50\n",
      "0s - loss: 0.0181 - acc: 0.9935 - val_loss: 2.9391 - val_acc: 0.8150\n",
      "Epoch 38/50\n",
      "0s - loss: 0.0142 - acc: 0.9965 - val_loss: 2.9004 - val_acc: 0.8175\n",
      "Epoch 39/50\n",
      "0s - loss: 0.0267 - acc: 0.9950 - val_loss: 2.8134 - val_acc: 0.8237\n",
      "Epoch 40/50\n",
      "0s - loss: 0.0149 - acc: 0.9945 - val_loss: 2.7561 - val_acc: 0.8275\n",
      "Epoch 41/50\n",
      "0s - loss: 0.0213 - acc: 0.9940 - val_loss: 2.6151 - val_acc: 0.8350\n",
      "Epoch 42/50\n",
      "0s - loss: 0.0077 - acc: 0.9970 - val_loss: 2.5611 - val_acc: 0.8375\n",
      "Epoch 43/50\n",
      "0s - loss: 0.0111 - acc: 0.9945 - val_loss: 2.8865 - val_acc: 0.8175\n",
      "Epoch 44/50\n",
      "0s - loss: 0.0184 - acc: 0.9950 - val_loss: 2.7332 - val_acc: 0.8275\n",
      "Epoch 45/50\n",
      "0s - loss: 0.0153 - acc: 0.9955 - val_loss: 2.4812 - val_acc: 0.8450\n",
      "Epoch 46/50\n",
      "0s - loss: 0.0145 - acc: 0.9960 - val_loss: 2.6850 - val_acc: 0.8313\n",
      "Epoch 47/50\n",
      "0s - loss: 0.0167 - acc: 0.9955 - val_loss: 2.5464 - val_acc: 0.8400\n",
      "Epoch 48/50\n",
      "0s - loss: 0.0094 - acc: 0.9975 - val_loss: 2.7314 - val_acc: 0.8287\n",
      "Epoch 49/50\n",
      "0s - loss: 0.0079 - acc: 0.9980 - val_loss: 2.7916 - val_acc: 0.8237\n",
      "Epoch 50/50\n",
      "0s - loss: 0.0097 - acc: 0.9955 - val_loss: 3.1837 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "with open('bottleneck_features_train.npy', 'rb') as features_train:\n",
    "    train_data = np.load(features_train)\n",
    "\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "train_labels = np.array([0] * 1000 + [1] * 1000)\n",
    "\n",
    "with open('bottleneck_features_validation.npy', 'rb') as features_validation:\n",
    "    validation_data = np.load(features_validation)\n",
    "validation_labels = np.array([0] * 400 + [1] * 400)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          \n",
    "          epochs=50, verbose=2,\n",
    "          batch_size=batch_size,\n",
    "          #steps_per_epoch=(train_data.shape[0] // batch_size),\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "          #validation_steps=(validation_data.shape[0] // batch_size))\n",
    "\n",
    "model.save_weights('bottleneck_fc_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "vgg16model = applications.VGG16(weights='imagenet', include_top=False,\n",
    "                               input_shape=(150,150,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tunning\n",
    "top_model.load_weights('bottleneck_fc_model.h5')\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "#vgg16model.add(top_model)\n",
    "# since the line above crashes with \"'Model' object has not attribute 'add'\"\n",
    "# we try the following instead\n",
    "model = Model(inputs=vgg16model.input, outputs=top_model(vgg16model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "# set the first 25 layers (up to t he last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "27s - loss: 0.8670 - acc: 0.8965 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 2/50\n",
      "27s - loss: 0.9299 - acc: 0.8905 - val_loss: 1.0245 - val_acc: 0.8938\n",
      "Epoch 3/50\n",
      "27s - loss: 0.9257 - acc: 0.8805 - val_loss: 1.0246 - val_acc: 0.8850\n",
      "Epoch 4/50\n",
      "27s - loss: 0.9433 - acc: 0.8865 - val_loss: 1.0837 - val_acc: 0.8862\n",
      "Epoch 5/50\n",
      "27s - loss: 0.8761 - acc: 0.8880 - val_loss: 1.0127 - val_acc: 0.8925\n",
      "Epoch 6/50\n",
      "27s - loss: 0.7607 - acc: 0.9030 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 7/50\n",
      "27s - loss: 0.8917 - acc: 0.8875 - val_loss: 1.0476 - val_acc: 0.8850\n",
      "Epoch 8/50\n",
      "27s - loss: 0.8720 - acc: 0.8955 - val_loss: 1.0306 - val_acc: 0.8938\n",
      "Epoch 9/50\n",
      "27s - loss: 0.9738 - acc: 0.8890 - val_loss: 1.0960 - val_acc: 0.8825\n",
      "Epoch 10/50\n",
      "27s - loss: 0.9056 - acc: 0.8830 - val_loss: 1.0664 - val_acc: 0.8875\n",
      "Epoch 11/50\n",
      "27s - loss: 0.7629 - acc: 0.9015 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 12/50\n",
      "27s - loss: 0.8748 - acc: 0.8950 - val_loss: 1.0314 - val_acc: 0.8925\n",
      "Epoch 13/50\n",
      "27s - loss: 0.8677 - acc: 0.8930 - val_loss: 1.0885 - val_acc: 0.8888\n",
      "Epoch 14/50\n",
      "27s - loss: 0.8909 - acc: 0.8935 - val_loss: 1.0606 - val_acc: 0.8875\n",
      "Epoch 15/50\n",
      "27s - loss: 0.9179 - acc: 0.8890 - val_loss: 1.0202 - val_acc: 0.8888\n",
      "Epoch 16/50\n",
      "27s - loss: 0.8645 - acc: 0.8875 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 17/50\n",
      "27s - loss: 0.8811 - acc: 0.8915 - val_loss: 1.0014 - val_acc: 0.8900\n",
      "Epoch 18/50\n",
      "27s - loss: 0.8804 - acc: 0.8870 - val_loss: 0.9641 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "27s - loss: 0.8567 - acc: 0.8920 - val_loss: 0.9321 - val_acc: 0.8950\n",
      "Epoch 20/50\n",
      "27s - loss: 0.9150 - acc: 0.8845 - val_loss: 1.0919 - val_acc: 0.8888\n",
      "Epoch 21/50\n",
      "27s - loss: 0.9387 - acc: 0.8865 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 22/50\n",
      "27s - loss: 0.8934 - acc: 0.8875 - val_loss: 1.0552 - val_acc: 0.8850\n",
      "Epoch 23/50\n",
      "27s - loss: 0.9143 - acc: 0.8835 - val_loss: 0.9782 - val_acc: 0.8975\n",
      "Epoch 24/50\n",
      "27s - loss: 0.9681 - acc: 0.8880 - val_loss: 0.9119 - val_acc: 0.9025\n",
      "Epoch 25/50\n",
      "27s - loss: 0.9021 - acc: 0.8900 - val_loss: 1.1501 - val_acc: 0.8825\n",
      "Epoch 26/50\n",
      "27s - loss: 0.8744 - acc: 0.8865 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 27/50\n",
      "26s - loss: 0.9163 - acc: 0.8860 - val_loss: 0.9980 - val_acc: 0.8925\n",
      "Epoch 28/50\n",
      "26s - loss: 0.8117 - acc: 0.8970 - val_loss: 1.1472 - val_acc: 0.8800\n",
      "Epoch 29/50\n",
      "27s - loss: 0.8617 - acc: 0.8960 - val_loss: 0.9840 - val_acc: 0.8862\n",
      "Epoch 30/50\n",
      "27s - loss: 0.9392 - acc: 0.8890 - val_loss: 0.9819 - val_acc: 0.8962\n",
      "Epoch 31/50\n",
      "27s - loss: 0.8924 - acc: 0.8915 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 32/50\n",
      "27s - loss: 0.8820 - acc: 0.8850 - val_loss: 1.0184 - val_acc: 0.8912\n",
      "Epoch 33/50\n",
      "27s - loss: 0.9057 - acc: 0.8945 - val_loss: 1.0507 - val_acc: 0.8850\n",
      "Epoch 34/50\n",
      "27s - loss: 0.8835 - acc: 0.8925 - val_loss: 0.9723 - val_acc: 0.8912\n",
      "Epoch 35/50\n",
      "27s - loss: 0.8565 - acc: 0.8930 - val_loss: 1.1169 - val_acc: 0.8950\n",
      "Epoch 36/50\n",
      "27s - loss: 0.8855 - acc: 0.8975 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 37/50\n",
      "27s - loss: 0.8829 - acc: 0.8925 - val_loss: 0.9645 - val_acc: 0.8950\n",
      "Epoch 38/50\n",
      "27s - loss: 0.8388 - acc: 0.8960 - val_loss: 1.1199 - val_acc: 0.8750\n",
      "Epoch 39/50\n",
      "27s - loss: 1.0209 - acc: 0.8835 - val_loss: 1.1350 - val_acc: 0.8812\n",
      "Epoch 40/50\n",
      "27s - loss: 0.9207 - acc: 0.8910 - val_loss: 1.0087 - val_acc: 0.8900\n",
      "Epoch 41/50\n",
      "27s - loss: 0.9259 - acc: 0.8925 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 42/50\n",
      "27s - loss: 0.8222 - acc: 0.8970 - val_loss: 1.0825 - val_acc: 0.8875\n",
      "Epoch 43/50\n",
      "27s - loss: 0.8822 - acc: 0.8915 - val_loss: 1.1251 - val_acc: 0.8788\n",
      "Epoch 44/50\n",
      "27s - loss: 0.8392 - acc: 0.8925 - val_loss: 1.0470 - val_acc: 0.8875\n",
      "Epoch 45/50\n",
      "27s - loss: 0.8884 - acc: 0.8900 - val_loss: 1.0931 - val_acc: 0.8888\n",
      "Epoch 46/50\n",
      "27s - loss: 0.8148 - acc: 0.8995 - val_loss: 1.0482 - val_acc: 0.8875\n",
      "Epoch 47/50\n",
      "27s - loss: 0.8238 - acc: 0.8895 - val_loss: 1.1185 - val_acc: 0.8788\n",
      "Epoch 48/50\n",
      "27s - loss: 0.9011 - acc: 0.8860 - val_loss: 0.9678 - val_acc: 0.9012\n",
      "Epoch 49/50\n",
      "27s - loss: 0.8867 - acc: 0.8945 - val_loss: 1.1520 - val_acc: 0.8750\n",
      "Epoch 50/50\n",
      "27s - loss: 0.8548 - acc: 0.8930 - val_loss: 1.0155 - val_acc: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69ac317cc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'data/validation',\n",
    "    target_size=(150,150),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2000//batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=800//batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more approaches you can try to get to above 0.95:\n",
    "    * more aggresive data augmentation\n",
    "    * more aggresive dropout\n",
    "    * use of L1 and L2 regularization (also known as \"weight decay\")\n",
    "    * fine-tuning one more convolutional block (alongside greater regularization)\n",
    " \n",
    " Here is where you can fine the code for our examples:\n",
    " \n",
    "     * Convnet trained from scratch:\n",
    "     https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "     \n",
    "     * Bottleneck features\n",
    "     https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n",
    "     \n",
    "     *Fine-tuning\n",
    "     https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
